{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LoggingUtil Documentation","text":"<p>Welcome to LoggingUtil, a powerful Python logging utility that surpasses the standard library logging module.</p>"},{"location":"#navigation","title":"Navigation","text":"<ul> <li>API Reference: Detailed API documentation<ul> <li>LogFile Class</li> <li>Handlers</li> </ul> </li> <li>Cloud Integration Guide: AWS and Elasticsearch integration guides</li> <li>Changelog: Version history and updates</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Advanced Handlers: Console, SQLite, Webhook, Email, File Rotation, CloudWatch, Elasticsearch</li> <li>Context Management: Add context to logs with correlation IDs and custom fields</li> <li>Structured Logging: Support for JSON and structured log formats</li> <li>Cloud Integration: Native support for AWS CloudWatch and Elasticsearch</li> <li>Performance: Batching, sampling, and async logging support</li> <li>Security: Automatic sensitive data redaction and secure credential handling</li> <li>Flexibility: Easy to extend with custom handlers and formatters</li> <li>Standard Library Compatible: Drop-in replacement for Python's logging module</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install loggingutil\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from loggingutil import LogFile, LogLevel\n\n# Create a logger\nlogger = LogFile(\"app.log\")\n\n# Log messages\nlogger.log(\"Hello world\", level=LogLevel.INFO)\n\n# Add context\nwith logger.context(user_id=\"123\"):\n    logger.log(\"User action\")\n\n# Track transactions\nwith logger.correlation(\"txn-456\"):\n    logger.log(\"Processing payment\")\n</code></pre>"},{"location":"#multiple-outputs","title":"Multiple Outputs","text":"<pre><code>from loggingutil.handlers import ConsoleHandler, ElasticsearchHandler\n\n# Add colored console output\nlogger.add_handler(ConsoleHandler(color=True))\n\n# Add Elasticsearch integration\nlogger.add_handler(ElasticsearchHandler(\n    \"http://elasticsearch:9200\",\n    index_prefix=\"myapp\"\n))\n</code></pre>"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#api-reference","title":"API Reference","text":"<p>Complete API documentation for all LoggingUtil classes and methods.</p>"},{"location":"#cloud-integration-guide","title":"Cloud Integration Guide","text":"<p>Detailed instructions for integrating with AWS CloudWatch, Elasticsearch, and other cloud services.</p>"},{"location":"#changelog","title":"Changelog","text":"<p>Track version history, updates, and breaking changes.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>GitHub Discussions: Ask questions and share ideas</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please check our GitHub repository for guidelines.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details. </p>"},{"location":"changelog/","title":"Changelog","text":"<p>Changelog</p> <p>This is a copy of the project's changelog. The original can be found in the repository root.</p>"},{"location":"changelog/#changelog","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#200-2024-03-21","title":"[2.0.0] - 2024-03-21","text":""},{"location":"changelog/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Complete overhaul of handler system with new implementations</li> <li>New configuration system supporting YAML, JSON, and environment variables</li> <li>Changed API for structured logging and context management</li> <li>Added mandatory correlation IDs for request tracking</li> <li>Modified log rotation and cleanup functionality</li> </ul>"},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Comprehensive handler implementations (Console, SQLite, Webhook, Email, File, CloudWatch, Elasticsearch)</li> <li>Advanced context management with correlation IDs</li> <li>Structured logging support with schema validation</li> <li>Log rotation and cleanup functionality</li> <li>Configuration system with YAML, JSON, and environment variable support</li> <li>CLI tool for log management</li> <li>Prometheus metrics integration</li> <li>Rich console output support</li> <li>Elasticsearch integration</li> <li>AWS CloudWatch integration</li> <li>MkDocs-based documentation</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Improved code quality with Black formatting</li> <li>Enhanced error handling and exception logging</li> <li>Simplified GitHub Actions workflow</li> <li>Streamlined CI pipeline</li> <li>Reorganized project structure</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Unused imports and code style issues</li> <li>Line length compliance</li> <li>Whitespace consistency</li> <li>Documentation formatting</li> </ul>"},{"location":"changelog/#123-2024-03-21","title":"[1.2.3] - 2024-03-21","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Comprehensive handler implementations (Console, SQLite, Webhook, Email, File, CloudWatch, Elasticsearch)</li> <li>Advanced context management with correlation IDs</li> <li>Structured logging support</li> <li>Log rotation and cleanup functionality</li> <li>Configuration system with YAML, JSON, and environment variable support</li> <li>CLI tool for log management</li> <li>Prometheus metrics integration</li> <li>Rich console output support</li> <li>Elasticsearch integration</li> <li>AWS CloudWatch integration</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Semi-full overhaul of all major functions</li> <li>Improved code quality with Black formatting</li> <li>Enhanced error handling and exception logging</li> <li>Simplified GitHub Actions workflow</li> <li>Streamlined CI pipeline</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Unused imports and code style issues</li> <li>Line length compliance</li> <li>Whitespace consistency</li> <li>Documentation formatting</li> </ul>"},{"location":"changelog/#122-initial-release","title":"[1.2.2] - Initial Release","text":"<ul> <li>Initial public release with basic functionality </li> </ul>"},{"location":"api/handlers/","title":"Handlers API Reference","text":""},{"location":"api/handlers/#base-handler","title":"Base Handler","text":""},{"location":"api/handlers/#loggingutil.handlers.BaseHandler","title":"<code>loggingutil.handlers.BaseHandler</code>","text":"<p>Base class for all handlers with common functionality.</p> Source code in <code>loggingutil\\handlers.py</code> <pre><code>class BaseHandler:\n    \"\"\"Base class for all handlers with common functionality.\"\"\"\n\n    async def handle(self, log_entry: dict) -&gt; None:\n        raise NotImplementedError\n</code></pre>"},{"location":"api/handlers/#console-handler","title":"Console Handler","text":""},{"location":"api/handlers/#loggingutil.handlers.ConsoleHandler","title":"<code>loggingutil.handlers.ConsoleHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler that prints logs to console with colors and formatting.</p> Source code in <code>loggingutil\\handlers.py</code> <pre><code>class ConsoleHandler(BaseHandler):\n    \"\"\"Handler that prints logs to console with colors and formatting.\"\"\"\n\n    COLORS = {\n        \"DEBUG\": \"\\033[36m\",  # Cyan\n        \"INFO\": \"\\033[32m\",  # Green\n        \"WARNING\": \"\\033[33m\",  # Yellow\n        \"ERROR\": \"\\033[31m\",  # Red\n        \"CRITICAL\": \"\\033[31;1m\",  # Bold Red\n        \"RESET\": \"\\033[0m\",\n    }\n\n    def __init__(self, color: bool = True, format: str = \"detailed\"):\n        self.use_color = color\n        self.format = format\n\n    async def handle(self, log_entry: dict):\n        timestamp = log_entry.get(\n            \"timestamp\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        )\n        level = log_entry.get(\"level\", \"INFO\")\n        data = log_entry.get(\"data\", \"\")\n        tag = log_entry.get(\"tag\", \"\")\n        correlation_id = log_entry.get(\"correlation_id\", \"\")\n\n        if self.use_color:\n            color = self.COLORS.get(level, \"\")\n            reset = self.COLORS[\"RESET\"]\n            message = f\"{timestamp} {color}[{level}]{reset}\"\n        else:\n            message = f\"{timestamp} [{level}]\"\n\n        if correlation_id:\n            message += f\" [{correlation_id}]\"\n        if tag:\n            message += f\" {tag}:\"\n        message += f\" {data}\"\n\n        print(message)\n</code></pre>"},{"location":"api/handlers/#example","title":"Example","text":"<pre><code>from loggingutil import LogFile\nfrom loggingutil.handlers import ConsoleHandler\n\nlogger = LogFile(\"app.log\")\n\n# Add colored console output\nlogger.add_handler(ConsoleHandler(\n    color=True,\n    format=\"detailed\"  # or \"simple\"\n))\n</code></pre>"},{"location":"api/handlers/#sqlite-handler","title":"SQLite Handler","text":""},{"location":"api/handlers/#loggingutil.handlers.SQLiteHandler","title":"<code>loggingutil.handlers.SQLiteHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler that stores logs in SQLite database.</p> Source code in <code>loggingutil\\handlers.py</code> <pre><code>class SQLiteHandler(BaseHandler):\n    \"\"\"Handler that stores logs in SQLite database.\"\"\"\n\n    def __init__(self, db_path: str = \"logs.db\"):\n        _warn_future_change(\n            \"SQLite schema will be enhanced with additional indexing and metadata columns\"\n        )\n        self.db_path = db_path\n        self._init_db()\n\n    def _init_db(self):\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n        c.execute(\n            \"\"\"CREATE TABLE IF NOT EXISTS logs\n                    (timestamp TEXT, level TEXT, tag TEXT,\n                     correlation_id TEXT, data TEXT)\"\"\"\n        )\n        conn.commit()\n        conn.close()\n\n    async def handle(self, log_entry: dict):\n        try:\n            conn = sqlite3.connect(self.db_path)\n            c = conn.cursor()\n            c.execute(\n                \"INSERT INTO logs VALUES (?, ?, ?, ?, ?)\",\n                (\n                    log_entry.get(\n                        \"timestamp\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                    ),\n                    log_entry.get(\"level\", \"INFO\"),\n                    log_entry.get(\"tag\"),\n                    log_entry.get(\"correlation_id\"),\n                    json.dumps(log_entry.get(\"data\", \"\")),\n                ),\n            )\n            conn.commit()\n            conn.close()\n        except Exception as e:\n            print(f\"SQLite error: {e}\")\n</code></pre>"},{"location":"api/handlers/#example_1","title":"Example","text":"<pre><code>from loggingutil.handlers import SQLiteHandler\n\nlogger.add_handler(SQLiteHandler(\"logs.db\"))\n\n# Query logs later\nimport sqlite3\nconn = sqlite3.connect(\"logs.db\")\ncursor = conn.execute(\"SELECT * FROM logs WHERE level = 'ERROR'\")\n</code></pre>"},{"location":"api/handlers/#webhook-handler","title":"Webhook Handler","text":""},{"location":"api/handlers/#loggingutil.handlers.WebhookHandler","title":"<code>loggingutil.handlers.WebhookHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler that sends logs to a webhook URL.</p> Source code in <code>loggingutil\\handlers.py</code> <pre><code>class WebhookHandler(BaseHandler):\n    \"\"\"Handler that sends logs to a webhook URL.\"\"\"\n\n    def __init__(self, webhook_url: str, batch_size: int = 10):\n        _warn_future_change(\n            \"Webhook handler will support additional retry and authentication options\"\n        )\n        self.webhook_url = webhook_url\n        self.batch_size = batch_size\n        self.batch: List[dict] = []\n\n    async def handle(self, log_entry: dict):\n        self.batch.append(log_entry)\n\n        if len(self.batch) &gt;= self.batch_size:\n            async with aiohttp.ClientSession() as session:\n                try:\n                    async with session.post(\n                        self.webhook_url, json={\"logs\": self.batch}\n                    ) as response:\n                        if response.status &gt;= 400:\n                            print(f\"Webhook error: {response.status}\")\n                finally:\n                    self.batch = []\n</code></pre>"},{"location":"api/handlers/#example_2","title":"Example","text":"<pre><code>from loggingutil.handlers import WebhookHandler\n\nlogger.add_handler(WebhookHandler(\n    webhook_url=\"https://api.example.com/logs\",\n    batch_size=10  # Send logs in batches of 10\n))\n</code></pre>"},{"location":"api/handlers/#email-handler","title":"Email Handler","text":""},{"location":"api/handlers/#loggingutil.handlers.EmailHandler","title":"<code>loggingutil.handlers.EmailHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler that sends critical logs via email.</p> Source code in <code>loggingutil\\handlers.py</code> <pre><code>class EmailHandler(BaseHandler):\n    \"\"\"Handler that sends critical logs via email.\"\"\"\n\n    def __init__(\n        self,\n        smtp_host: str,\n        smtp_port: int,\n        username: str,\n        password: str,\n        from_addr: str,\n        to_addrs: List[str],\n        min_level: str = \"ERROR\",\n    ):\n        self.smtp_host = smtp_host\n        self.smtp_port = smtp_port\n        self.username = username\n        self.password = password\n        self.from_addr = from_addr\n        self.to_addrs = to_addrs\n        self.min_level = min_level\n\n    async def handle(self, log_entry: dict):\n        if log_entry[\"level\"] &lt; self.min_level:\n            return\n\n        msg = EmailMessage()\n        msg.set_content(json.dumps(log_entry, indent=2))\n        msg[\"Subject\"] = (\n            f\"Log Alert: {log_entry['level']} - {log_entry.get('tag', 'No Tag')}\"\n        )\n        msg[\"From\"] = self.from_addr\n        msg[\"To\"] = \", \".join(self.to_addrs)\n\n        with smtplib.SMTP(self.smtp_host, self.smtp_port) as server:\n            server.starttls()\n            server.login(self.username, self.password)\n            server.send_message(msg)\n</code></pre>"},{"location":"api/handlers/#example_3","title":"Example","text":"<pre><code>from loggingutil.handlers import EmailHandler\n\nlogger.add_handler(EmailHandler(\n    smtp_host=\"smtp.gmail.com\",\n    smtp_port=587,\n    username=\"alerts@example.com\",\n    password=\"app_password\",\n    from_addr=\"alerts@example.com\",\n    to_addrs=[\"admin@example.com\"],\n    min_level=\"ERROR\"  # Only send ERROR and above\n))\n</code></pre>"},{"location":"api/handlers/#file-rotating-handler","title":"File Rotating Handler","text":""},{"location":"api/handlers/#loggingutil.handlers.FileRotatingHandler","title":"<code>loggingutil.handlers.FileRotatingHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler that writes logs to rotating files by date or tag.</p> Source code in <code>loggingutil\\handlers.py</code> <pre><code>class FileRotatingHandler(BaseHandler):\n    \"\"\"Handler that writes logs to rotating files by date or tag.\"\"\"\n\n    def __init__(\n        self,\n        base_dir: str = \"logs\",\n        rotate_by: str = \"date\",  # or \"tag\"\n        max_files: int = 30,\n    ):\n        _warn_future_change(\n            \"File rotation strategy will be enhanced with more granular controls\"\n        )\n        self.base_dir = Path(base_dir)\n        self.rotate_by = rotate_by\n        self.max_files = max_files\n        os.makedirs(self.base_dir, exist_ok=True)\n\n    def _get_filename(self, log_entry: dict) -&gt; Path:\n        if self.rotate_by == \"date\":\n            date_str = datetime.now().strftime(\"%Y-%m-%d\")\n            return self.base_dir / f\"{date_str}.log\"\n        else:  # rotate by tag\n            tag = log_entry.get(\"tag\", \"notag\")\n            return self.base_dir / f\"{tag}.log\"\n\n    async def handle(self, log_entry: dict):\n        try:\n            filepath = self._get_filename(log_entry)\n            with open(filepath, \"a\") as f:\n                f.write(json.dumps(log_entry) + \"\\n\")\n\n            # Cleanup old files if needed\n            await self._cleanup_old_files()\n        except Exception as e:\n            print(f\"File rotation error: {e}\")\n\n    async def _cleanup_old_files(self):\n        try:\n            files = sorted(self.base_dir.glob(\"*.log\"), key=lambda x: x.stat().st_mtime)\n            if len(files) &gt; self.max_files:\n                for f in files[: -self.max_files]:\n                    try:\n                        f.unlink()\n                    except OSError as e:\n                        print(f\"Error deleting old log file {f}: {e}\")\n        except Exception as e:\n            print(f\"Error during file cleanup: {e}\")\n</code></pre>"},{"location":"api/handlers/#example_4","title":"Example","text":"<pre><code>from loggingutil.handlers import FileRotatingHandler\n\nlogger.add_handler(FileRotatingHandler(\n    base_dir=\"logs\",\n    rotate_by=\"date\",  # or \"tag\"\n    max_files=30\n))\n</code></pre>"},{"location":"api/handlers/#cloudwatch-handler","title":"CloudWatch Handler","text":""},{"location":"api/handlers/#loggingutil.handlers.CloudWatchHandler","title":"<code>loggingutil.handlers.CloudWatchHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler that sends logs to AWS CloudWatch.</p> Source code in <code>loggingutil\\handlers.py</code> <pre><code>class CloudWatchHandler(BaseHandler):\n    \"\"\"Handler that sends logs to AWS CloudWatch.\"\"\"\n\n    def __init__(\n        self,\n        log_group: str,\n        log_stream: str,\n        aws_access_key: Optional[str] = None,\n        aws_secret_key: Optional[str] = None,\n        region: Optional[str] = None,\n    ):\n        try:\n            import boto3\n\n            self.client = boto3.client(\n                \"logs\",\n                aws_access_key_id=aws_access_key,\n                aws_secret_access_key=aws_secret_key,\n                region_name=region,\n            )\n        except ImportError:\n            raise ImportError(\"boto3 required for CloudWatch handler\")\n\n        self.log_group = log_group\n        self.log_stream = log_stream\n        self._ensure_log_group()\n\n    def _ensure_log_group(self):\n        try:\n            self.client.create_log_group(logGroupName=self.log_group)\n        except self.client.exceptions.ResourceAlreadyExistsException:\n            pass\n\n        try:\n            self.client.create_log_stream(\n                logGroupName=self.log_group, logStreamName=self.log_stream\n            )\n        except self.client.exceptions.ResourceAlreadyExistsException:\n            pass\n\n    async def handle(self, log_entry: dict):\n        event = {\n            \"timestamp\": int(datetime.now().timestamp() * 1000),\n            \"message\": json.dumps(log_entry),\n        }\n\n        try:\n            self.client.put_log_events(\n                logGroupName=self.log_group,\n                logStreamName=self.log_stream,\n                logEvents=[event],\n            )\n        except Exception as e:\n            print(f\"CloudWatch error: {e}\")\n</code></pre>"},{"location":"api/handlers/#example_5","title":"Example","text":"<pre><code>from loggingutil.handlers import CloudWatchHandler\n\nlogger.add_handler(CloudWatchHandler(\n    log_group=\"/myapp/prod\",\n    log_stream=\"api-server\",\n    aws_access_key=\"YOUR_ACCESS_KEY\",\n    aws_secret_key=\"YOUR_SECRET_KEY\",\n    region=\"us-west-2\"\n))\n</code></pre>"},{"location":"api/handlers/#elasticsearch-handler","title":"Elasticsearch Handler","text":""},{"location":"api/handlers/#loggingutil.handlers.ElasticsearchHandler","title":"<code>loggingutil.handlers.ElasticsearchHandler</code>","text":"<p>               Bases: <code>BaseHandler</code></p> <p>Handler that sends logs to Elasticsearch.</p> Source code in <code>loggingutil\\handlers.py</code> <pre><code>class ElasticsearchHandler(BaseHandler):\n    \"\"\"Handler that sends logs to Elasticsearch.\"\"\"\n\n    def __init__(\n        self, es_url: str, index_prefix: str = \"logs\", auth: Optional[tuple] = None\n    ):\n        self.es_url = es_url\n        self.index_prefix = index_prefix\n        self.auth = auth\n\n    def _get_index_name(self) -&gt; str:\n        return f\"{self.index_prefix}-{datetime.now().strftime('%Y.%m.%d')}\"\n\n    async def handle(self, log_entry: dict):\n        index = self._get_index_name()\n        url = f\"{self.es_url}/{index}/_doc\"\n\n        async with aiohttp.ClientSession() as session:\n            try:\n                async with session.post(url, json=log_entry, auth=self.auth) as resp:\n                    if resp.status &gt;= 400:\n                        print(f\"Elasticsearch error: {resp.status}\")\n            except Exception as e:\n                print(f\"Elasticsearch error: {e}\")\n</code></pre>"},{"location":"api/handlers/#example_6","title":"Example","text":"<pre><code>from loggingutil.handlers import ElasticsearchHandler\n\nlogger.add_handler(ElasticsearchHandler(\n    es_url=\"http://elasticsearch:9200\",\n    index_prefix=\"myapp-logs\",\n    auth=(\"user\", \"pass\")  # Optional authentication\n))\n</code></pre>"},{"location":"api/logfile/","title":"LogFile API Reference","text":""},{"location":"api/logfile/#loggingutil.LogFile","title":"<code>loggingutil.LogFile</code>","text":"<p>Enhanced logging utility that surpasses the standard library logging module.</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>class LogFile:\n    \"\"\"Enhanced logging utility that surpasses the standard library logging module.\"\"\"\n\n    def __init__(\n        self,\n        filename: str = \"logs.log\",\n        verbose: bool = True,\n        max_size_mb: int = 5,\n        keep_days: int = 7,\n        timestamp_format: str = \"[%Y-%m-%d %H:%M:%S.%f]\",\n        mode: str = \"json\",\n        compress: bool = False,\n        use_utc: bool = False,\n        include_timestamp: bool = True,\n        custom_formatter: Optional[Callable] = None,\n        external_stream: Optional[Callable[[str], None]] = None,\n        sampling_rate: float = 1.0,\n        batch_size: int = 100,\n        rotate_time: Optional[str] = None,  # \"daily\", \"hourly\", None\n        sanitize_keys: List[str] = None,\n        schema_validation: bool = False,\n    ):\n        if custom_formatter:\n            _warn_future_change(\n                \"Custom formatter API will be enhanced with additional context parameters.\"\n            )\n\n        if not rotate_time and max_size_mb &gt; 0:\n            _warn_future_change(\n                \"Default log rotation behavior will be changed to prefer time-based rotation.\"\n            )\n\n        if not sanitize_keys:\n            _warn_future_change(\n                \"Default sensitive data protection will be enhanced in future versions.\"\n            )\n\n        self.filename = filename\n        self.verbose = verbose\n        self.max_size = max_size_mb * 1024 * 1024\n        self.keep_days = keep_days\n        self.timestamp_format = timestamp_format\n        self.mode = mode.lower()\n        self.compress = compress\n        self.use_utc = use_utc\n        self.include_timestamp = include_timestamp\n        self.custom_formatter = custom_formatter\n        self.external_stream = external_stream\n        self.sampling_rate = max(0.0, min(1.0, sampling_rate))\n        self.batch_size = batch_size\n        self.rotate_time = rotate_time\n        self.sanitize_keys = sanitize_keys or [\"password\", \"token\", \"secret\", \"key\"]\n        self.schema_validation = schema_validation\n\n        self.buffer = []\n        self.buffer_limit = batch_size\n        self.level = LogLevel.INFO\n        self.handlers: List[LogHandler] = []\n        self.filters: List[LogFilter] = []\n        self.metrics = LogMetrics()\n        self._last_rotation_check = datetime.now()\n        self._correlation_id = None\n\n        self.loadfile()\n        self.cleanup_old_logs()\n\n    @property\n    def correlation_id(self) -&gt; str:\n        \"\"\"Get current correlation ID or generate a new one\"\"\"\n        if not self._correlation_id:\n            self._correlation_id = str(uuid.uuid4())\n        return self._correlation_id\n\n    @correlation_id.setter\n    def correlation_id(self, value: str):\n        self._correlation_id = value\n\n    def add_handler(self, handler: LogHandler):\n        \"\"\"Add a custom log handler\"\"\"\n        self.handlers.append(handler)\n\n    def add_filter(self, log_filter: LogFilter):\n        \"\"\"Add a custom log filter\"\"\"\n        self.filters.append(log_filter)\n\n    def should_sample(self) -&gt; bool:\n        \"\"\"Check if this log entry should be sampled.\"\"\"\n        return self.sampling_rate &gt;= 1.0 or random.random() &lt; self.sampling_rate\n\n    def _sanitize_data(self, data: Any) -&gt; Any:\n        \"\"\"Recursively sanitize sensitive data\"\"\"\n        if isinstance(data, dict):\n            return {\n                k: (\n                    \"***REDACTED***\"\n                    if k in self.sanitize_keys\n                    else self._sanitize_data(v)\n                )\n                for k, v in data.items()\n            }\n        elif isinstance(data, list):\n            return [self._sanitize_data(item) for item in data]\n        return data\n\n    def _get_caller_info(self) -&gt; dict:\n        \"\"\"Get information about the calling function\"\"\"\n        stack = inspect.stack()\n        # Skip this function and internal logging functions\n        for frame_info in stack[2:]:\n            if \"logging\" not in frame_info.filename:\n                return {\n                    \"file\": os.path.basename(frame_info.filename),\n                    \"function\": frame_info.function,\n                    \"line\": frame_info.lineno,\n                }\n        return {}\n\n    def format_log_entry(\n        self,\n        timestamp: str,\n        level: LogLevel,\n        tag: str,\n        data: Any,\n        context_str: str,\n    ) -&gt; str:\n        \"\"\"Format a log entry as a string.\"\"\"\n        if self.mode == \"json\":\n            entry = {\n                \"timestamp\": timestamp,\n                \"level\": level.name,\n                \"correlation_id\": self.correlation_id,\n                \"tag\": tag,\n                \"data\": data,\n            }\n            if context_str:\n                try:\n                    entry[\"context\"] = dict(\n                        item.split(\"=\") for item in context_str.split()\n                    )\n                except (ValueError, AttributeError):\n                    entry[\"context\"] = context_str\n            return json.dumps(entry) + \"\\n\"\n        else:\n            return (\n                f\"{timestamp} [{level.name}] [{self.correlation_id}] \"\n                f\"{tag or ''} {context_str} {data}\\n\"\n            )\n\n    async def _handle_async(self, entry: dict):\n        \"\"\"Process log entry through async handlers\"\"\"\n        for handler in self.handlers:\n            try:\n                await handler.handle(entry)\n            except Exception as e:\n                print(f\"Handler error: {e}\")\n\n    @contextmanager\n    def context(self, **kwargs):\n        \"\"\"Context manager for adding temporary context to logs\"\"\"\n        previous = dict(LogContext.get_context())\n        LogContext.set(**kwargs)\n        try:\n            yield\n        finally:\n            LogContext._context.data = previous\n\n    @contextmanager\n    def correlation(self, correlation_id: str):\n        \"\"\"Context manager for setting correlation ID\"\"\"\n        previous = self._correlation_id\n        self.correlation_id = correlation_id\n        try:\n            yield\n        finally:\n            self.correlation_id = previous\n\n    def get_metrics(self) -&gt; dict:\n        \"\"\"Get current logging metrics\"\"\"\n        return self.metrics.get_stats()\n\n    def structured(self, **kwargs):\n        \"\"\"Log structured data with schema validation\"\"\"\n        if not self.schema_validation:\n            _warn_future_change(\n                \"Schema validation will be enabled by default for structured logging.\"\n            )\n        self.log(kwargs)\n\n    def _print(self, msg):\n        if self.verbose:\n            print(f\"LoggingUtility :: {msg}\")\n\n    def loadfile(self):\n        \"\"\"Initialize/create log file if it is destroyed.\"\"\"\n        if not os.path.exists(self.filename):\n            init_entry = self.format_log_entry(\n                self._get_timestamp(),\n                LogLevel.INFO,\n                \"INIT\",\n                \"Log file initialized\",\n                \"\",\n            )\n            with open(self.filename, \"w\") as f:\n                f.write(init_entry)\n            self._print(f\"Created new log file: {self.filename}\")\n\n    def setLevel(self, level: LogLevel) -&gt; None:\n        \"\"\"Sets default output level (used if no level passed to .log())\n\n        Example:\n        logfile = loggingutil.LogFile()\n        logfile.setLevel(logfile.notice)\n        \"\"\"\n        if isinstance(level, LogLevel):\n            self.level = level\n\n    def getLevel(self):\n        return self.level\n\n    def levelEquiv(self, level):\n        return level.name if isinstance(level, LogLevel) else str(level)\n\n    def _rotate_if_needed(self):\n        if (\n            os.path.exists(self.filename)\n            and os.path.getsize(self.filename) &gt;= self.max_size\n        ):\n            timestamp = (\n                datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n                if self.use_utc\n                else datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            )\n            base, ext = os.path.splitext(self.filename)\n            rotated_name = (\n                f\"{base}_{timestamp}{ext}.gz\"\n                if self.compress\n                else f\"{base}_{timestamp}{ext}\"\n            )\n            with open(self.filename, \"rb\") as f_in:\n                with (\n                    gzip.open(rotated_name, \"wb\")\n                    if self.compress\n                    else open(rotated_name, \"wb\")\n                ) as f_out:\n                    f_out.write(f_in.read())\n            os.remove(self.filename)\n            self._print(f\"Log rotated: {rotated_name}\")\n\n    def cleanup_old_logs(self):\n        base, _ = os.path.splitext(self.filename)\n        now = datetime.utcnow() if self.use_utc else datetime.now()\n        for file in os.listdir(\".\"):\n            if file.startswith(base + \"_\"):\n                try:\n                    timestamp_str = file.split(\"_\")[-1].split(\".\")[0]\n                    file_time = datetime.strptime(timestamp_str, \"%Y%m%d_%H%M%S\")\n                    if now - file_time &gt; timedelta(days=self.keep_days):\n                        os.remove(file)\n                        self._print(f\"Deleted old log file: {file}\")\n                except Exception:\n                    continue\n\n    def _get_timestamp(self):\n        now = datetime.utcnow() if self.use_utc else datetime.now()\n        return now.strftime(self.timestamp_format) if self.include_timestamp else \"\"\n\n    def _check_time_rotation(self):\n        \"\"\"Check if log file should be rotated based on time\"\"\"\n        if not self.rotate_time:\n            return\n\n        now = datetime.now()\n        if self.rotate_time == \"daily\":\n            if now.date() &gt; self._last_rotation_check.date():\n                self._rotate_log()\n        elif self.rotate_time == \"hourly\":\n            if now.hour &gt; self._last_rotation_check.hour:\n                self._rotate_log()\n\n        self._last_rotation_check = now\n\n    def _rotate_log(self):\n        \"\"\"Rotate log file\"\"\"\n        if not os.path.exists(self.filename):\n            return\n\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        base, ext = os.path.splitext(self.filename)\n        rotated_name = f\"{base}_{timestamp}{ext}\"\n\n        if self.compress:\n            rotated_name += \".gz\"\n            with open(self.filename, \"rb\") as f_in:\n                with gzip.open(rotated_name, \"wb\") as f_out:\n                    f_out.write(f_in.read())\n        else:\n            os.rename(self.filename, rotated_name)\n\n        with open(self.filename, \"w\") as f:\n            f.write(f\"Log file rotated from {rotated_name}\\n\")\n\n        self._print(f\"Log rotated: {rotated_name}\")\n\n    def log(self, data: Any, level: LogLevel = None, tag: str = None):\n        \"\"\"Enhanced log method with filtering and metrics\"\"\"\n        if level is None:\n            _warn_future_change(\n                \"Default log level behavior will be more strict. Please specify a level explicitly.\"\n            )\n            level = self.level\n        elif isinstance(level, str):\n            _warn_future_change(\n                \"Using string log levels will be deprecated. Please use LogLevel enum.\"\n            )\n            try:\n                level = LogLevel[level.upper()]\n            except (KeyError, AttributeError):\n                level = self.level\n\n        if not self.should_sample():\n            return\n\n        if not data:\n            raise ValueError(\"No data provided\")\n\n        # Apply filters\n        log_entry = {\n            \"level\": level,\n            \"tag\": tag,\n            \"data\": data,\n            \"timestamp\": self._get_timestamp(),\n        }\n\n        for log_filter in self.filters:\n            if not log_filter.filter(log_entry):\n                return\n\n        entry = self.format_log_entry(\n            self._get_timestamp(),\n            level,\n            tag,\n            self._sanitize_data(data),\n            \" \".join(f\"{k}={v}\" for k, v in LogContext.get_context().items()),\n        )\n        self.buffer.append(entry)\n        self.metrics.record_log(level)\n\n        # Flush immediately instead of waiting for buffer to fill\n        self.flush()\n\n        # Check for time-based rotation\n        self._check_time_rotation()\n\n    async def async_batch_log(self, entries: List[tuple]):\n        \"\"\"Log multiple entries asynchronously\"\"\"\n\n        async def process_batch(batch):\n            for data, level, tag in batch:\n                await self.async_log(data, level, tag)\n\n        batch_size = self.batch_size\n        for i in range(0, len(entries), batch_size):\n            batch = entries[i : i + batch_size]\n            await process_batch(batch)\n\n    def flush(self):\n        \"\"\"Clears the buffer and dumps current buffer data to log file.\"\"\"\n        if not self.buffer:\n            return\n\n        for entry in self.buffer:\n            self._write(entry)\n        self.buffer.clear()\n        self._print(\"Buffer flushed to file.\")\n\n    async def async_log(self, data, level: LogLevel = None, tag=None):\n        if level is None:\n            level = self.getLevel()\n\n        \"\"\"Coroutine log function\"\"\"\n        self.log(data, level, tag)\n\n    async def async_log_http_response(self, resp, level: LogLevel = None, tag=\"HTTP\"):\n        \"\"\"Log HTTP responses from APIs\"\"\"\n        if level == None:\n            level = self.getLevel()\n        try:\n            info = {\n                \"status\": resp.status,\n                \"headers\": dict(resp.headers),\n                \"body\": await resp.text(),\n            }\n            self.log(info, level=level, tag=tag)\n        except Exception as e:\n            self.log_exception(e)\n\n    def log_exception(self, err, tag=\"EXCEPTION\"):\n        \"\"\"For logging specifically exceptions as errors.\"\"\"\n        tb = traceback.format_exc()\n        data = {\"error\": str(err), \"traceback\": tb}\n        self.log(data, level=LogLevel.ERROR, tag=tag)\n\n    def wipe(self):\n        \"\"\"Completely clear the log file.\"\"\"\n        self.flush()\n        with open(self.filename, \"w\"):\n            pass\n        self._print(f\"File {self.filename} has been wiped.\")\n\n    def __enter__(self):\n        self._correlation_id = str(uuid.uuid4())\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.flush()\n        if exc_type:\n            self.log_exception(exc_val)\n        self._correlation_id = None\n\n    def __repr__(self):\n        return f\"&lt;LogFile filename='{self.filename}' mode='{self.mode}' level='{self.levelEquiv(self.getLevel())}'&gt;\"\n\n    def __str__(self):\n        return f\"LogFile({self.filename})\"\n\n    def increment_error_count(self, error_type: str) -&gt; None:\n        \"\"\"Increment error count for a specific error type.\"\"\"\n        self.metrics.record_error(error_type)\n\n    def _parse_timestamp(self, timestamp_str: str) -&gt; datetime:\n        \"\"\"Parse timestamp from filename.\"\"\"\n        file_time = datetime.strptime(timestamp_str, \"%Y%m%d_%H%M%S\")\n        return file_time\n\n    def _format_timestamp(self) -&gt; str:\n        \"\"\"Format current timestamp.\"\"\"\n        now = datetime.now()\n        return now.strftime(self.timestamp_format) if self.include_timestamp else \"\"\n\n    def _process_batch(self, entries: List[dict], batch_size: int = 100):\n        \"\"\"Process a batch of log entries.\"\"\"\n        for i in range(0, len(entries), batch_size):\n            batch = entries[i : i + batch_size]\n            # ... rest of the method\n\n    def _write(self, entry: str):\n        \"\"\"Write a log entry to the file.\"\"\"\n        with open(self.filename, \"a\") as f:\n            f.write(entry)\n        if self.external_stream:\n            self.external_stream(entry)\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.correlation_id","title":"<code>correlation_id</code>  <code>property</code> <code>writable</code>","text":"<p>Get current correlation ID or generate a new one</p>"},{"location":"api/logfile/#loggingutil.LogFile.add_filter","title":"<code>add_filter(log_filter)</code>","text":"<p>Add a custom log filter</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def add_filter(self, log_filter: LogFilter):\n    \"\"\"Add a custom log filter\"\"\"\n    self.filters.append(log_filter)\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.add_handler","title":"<code>add_handler(handler)</code>","text":"<p>Add a custom log handler</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def add_handler(self, handler: LogHandler):\n    \"\"\"Add a custom log handler\"\"\"\n    self.handlers.append(handler)\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.async_batch_log","title":"<code>async_batch_log(entries)</code>  <code>async</code>","text":"<p>Log multiple entries asynchronously</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>async def async_batch_log(self, entries: List[tuple]):\n    \"\"\"Log multiple entries asynchronously\"\"\"\n\n    async def process_batch(batch):\n        for data, level, tag in batch:\n            await self.async_log(data, level, tag)\n\n    batch_size = self.batch_size\n    for i in range(0, len(entries), batch_size):\n        batch = entries[i : i + batch_size]\n        await process_batch(batch)\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.async_log_http_response","title":"<code>async_log_http_response(resp, level=None, tag='HTTP')</code>  <code>async</code>","text":"<p>Log HTTP responses from APIs</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>async def async_log_http_response(self, resp, level: LogLevel = None, tag=\"HTTP\"):\n    \"\"\"Log HTTP responses from APIs\"\"\"\n    if level == None:\n        level = self.getLevel()\n    try:\n        info = {\n            \"status\": resp.status,\n            \"headers\": dict(resp.headers),\n            \"body\": await resp.text(),\n        }\n        self.log(info, level=level, tag=tag)\n    except Exception as e:\n        self.log_exception(e)\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.context","title":"<code>context(**kwargs)</code>","text":"<p>Context manager for adding temporary context to logs</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>@contextmanager\ndef context(self, **kwargs):\n    \"\"\"Context manager for adding temporary context to logs\"\"\"\n    previous = dict(LogContext.get_context())\n    LogContext.set(**kwargs)\n    try:\n        yield\n    finally:\n        LogContext._context.data = previous\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.correlation","title":"<code>correlation(correlation_id)</code>","text":"<p>Context manager for setting correlation ID</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>@contextmanager\ndef correlation(self, correlation_id: str):\n    \"\"\"Context manager for setting correlation ID\"\"\"\n    previous = self._correlation_id\n    self.correlation_id = correlation_id\n    try:\n        yield\n    finally:\n        self.correlation_id = previous\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.flush","title":"<code>flush()</code>","text":"<p>Clears the buffer and dumps current buffer data to log file.</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def flush(self):\n    \"\"\"Clears the buffer and dumps current buffer data to log file.\"\"\"\n    if not self.buffer:\n        return\n\n    for entry in self.buffer:\n        self._write(entry)\n    self.buffer.clear()\n    self._print(\"Buffer flushed to file.\")\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.format_log_entry","title":"<code>format_log_entry(timestamp, level, tag, data, context_str)</code>","text":"<p>Format a log entry as a string.</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def format_log_entry(\n    self,\n    timestamp: str,\n    level: LogLevel,\n    tag: str,\n    data: Any,\n    context_str: str,\n) -&gt; str:\n    \"\"\"Format a log entry as a string.\"\"\"\n    if self.mode == \"json\":\n        entry = {\n            \"timestamp\": timestamp,\n            \"level\": level.name,\n            \"correlation_id\": self.correlation_id,\n            \"tag\": tag,\n            \"data\": data,\n        }\n        if context_str:\n            try:\n                entry[\"context\"] = dict(\n                    item.split(\"=\") for item in context_str.split()\n                )\n            except (ValueError, AttributeError):\n                entry[\"context\"] = context_str\n        return json.dumps(entry) + \"\\n\"\n    else:\n        return (\n            f\"{timestamp} [{level.name}] [{self.correlation_id}] \"\n            f\"{tag or ''} {context_str} {data}\\n\"\n        )\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get current logging metrics</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def get_metrics(self) -&gt; dict:\n    \"\"\"Get current logging metrics\"\"\"\n    return self.metrics.get_stats()\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.increment_error_count","title":"<code>increment_error_count(error_type)</code>","text":"<p>Increment error count for a specific error type.</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def increment_error_count(self, error_type: str) -&gt; None:\n    \"\"\"Increment error count for a specific error type.\"\"\"\n    self.metrics.record_error(error_type)\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.loadfile","title":"<code>loadfile()</code>","text":"<p>Initialize/create log file if it is destroyed.</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def loadfile(self):\n    \"\"\"Initialize/create log file if it is destroyed.\"\"\"\n    if not os.path.exists(self.filename):\n        init_entry = self.format_log_entry(\n            self._get_timestamp(),\n            LogLevel.INFO,\n            \"INIT\",\n            \"Log file initialized\",\n            \"\",\n        )\n        with open(self.filename, \"w\") as f:\n            f.write(init_entry)\n        self._print(f\"Created new log file: {self.filename}\")\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.log","title":"<code>log(data, level=None, tag=None)</code>","text":"<p>Enhanced log method with filtering and metrics</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def log(self, data: Any, level: LogLevel = None, tag: str = None):\n    \"\"\"Enhanced log method with filtering and metrics\"\"\"\n    if level is None:\n        _warn_future_change(\n            \"Default log level behavior will be more strict. Please specify a level explicitly.\"\n        )\n        level = self.level\n    elif isinstance(level, str):\n        _warn_future_change(\n            \"Using string log levels will be deprecated. Please use LogLevel enum.\"\n        )\n        try:\n            level = LogLevel[level.upper()]\n        except (KeyError, AttributeError):\n            level = self.level\n\n    if not self.should_sample():\n        return\n\n    if not data:\n        raise ValueError(\"No data provided\")\n\n    # Apply filters\n    log_entry = {\n        \"level\": level,\n        \"tag\": tag,\n        \"data\": data,\n        \"timestamp\": self._get_timestamp(),\n    }\n\n    for log_filter in self.filters:\n        if not log_filter.filter(log_entry):\n            return\n\n    entry = self.format_log_entry(\n        self._get_timestamp(),\n        level,\n        tag,\n        self._sanitize_data(data),\n        \" \".join(f\"{k}={v}\" for k, v in LogContext.get_context().items()),\n    )\n    self.buffer.append(entry)\n    self.metrics.record_log(level)\n\n    # Flush immediately instead of waiting for buffer to fill\n    self.flush()\n\n    # Check for time-based rotation\n    self._check_time_rotation()\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.log_exception","title":"<code>log_exception(err, tag='EXCEPTION')</code>","text":"<p>For logging specifically exceptions as errors.</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def log_exception(self, err, tag=\"EXCEPTION\"):\n    \"\"\"For logging specifically exceptions as errors.\"\"\"\n    tb = traceback.format_exc()\n    data = {\"error\": str(err), \"traceback\": tb}\n    self.log(data, level=LogLevel.ERROR, tag=tag)\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.setLevel","title":"<code>setLevel(level)</code>","text":"<p>Sets default output level (used if no level passed to .log())</p> <p>Example: logfile = loggingutil.LogFile() logfile.setLevel(logfile.notice)</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def setLevel(self, level: LogLevel) -&gt; None:\n    \"\"\"Sets default output level (used if no level passed to .log())\n\n    Example:\n    logfile = loggingutil.LogFile()\n    logfile.setLevel(logfile.notice)\n    \"\"\"\n    if isinstance(level, LogLevel):\n        self.level = level\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.should_sample","title":"<code>should_sample()</code>","text":"<p>Check if this log entry should be sampled.</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def should_sample(self) -&gt; bool:\n    \"\"\"Check if this log entry should be sampled.\"\"\"\n    return self.sampling_rate &gt;= 1.0 or random.random() &lt; self.sampling_rate\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.structured","title":"<code>structured(**kwargs)</code>","text":"<p>Log structured data with schema validation</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def structured(self, **kwargs):\n    \"\"\"Log structured data with schema validation\"\"\"\n    if not self.schema_validation:\n        _warn_future_change(\n            \"Schema validation will be enabled by default for structured logging.\"\n        )\n    self.log(kwargs)\n</code></pre>"},{"location":"api/logfile/#loggingutil.LogFile.wipe","title":"<code>wipe()</code>","text":"<p>Completely clear the log file.</p> Source code in <code>loggingutil\\__init__.py</code> <pre><code>def wipe(self):\n    \"\"\"Completely clear the log file.\"\"\"\n    self.flush()\n    with open(self.filename, \"w\"):\n        pass\n    self._print(f\"File {self.filename} has been wiped.\")\n</code></pre>"},{"location":"api/logfile/#examples","title":"Examples","text":""},{"location":"api/logfile/#basic-usage","title":"Basic Usage","text":"<pre><code>from loggingutil import LogFile, LogLevel\n\n# Create a logger\nlogger = LogFile(\"app.log\")\n\n# Log messages with different levels\nlogger.log(\"Hello world\", level=LogLevel.INFO)\nlogger.log(\"Debug info\", level=LogLevel.DEBUG)\nlogger.log(\"Warning!\", level=LogLevel.WARN)\n\n# Use context managers\nwith logger.context(user_id=\"123\"):\n    logger.log(\"User action\")\n\n# Use correlation IDs\nwith logger.correlation(\"txn-456\"):\n    logger.log(\"Transaction processing\")\n\n# Log structured data\nlogger.structured(\n    event=\"user_login\",\n    user_id=\"123\",\n    ip=\"192.168.1.1\"\n)\n\n# Log exceptions\ntry:\n    raise ValueError(\"Something went wrong\")\nexcept Exception as e:\n    logger.log_exception(e)\n</code></pre>"},{"location":"api/logfile/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>logger = LogFile(\n    filename=\"app.log\",\n    mode=\"json\",\n    level=LogLevel.INFO,\n    rotate_time=\"daily\",\n    max_size_mb=100,\n    keep_days=30,\n    compress=True,\n    batch_size=100,\n    sampling_rate=0.1,\n    sanitize_keys=[\"password\", \"token\"],\n    use_utc=True\n)\n</code></pre>"},{"location":"api/logfile/#using-multiple-handlers","title":"Using Multiple Handlers","text":"<pre><code>from loggingutil.handlers import ConsoleHandler, ElasticsearchHandler\n\nlogger = LogFile(\"app.log\")\n\n# Add console output with colors\nlogger.add_handler(ConsoleHandler(color=True))\n\n# Add Elasticsearch integration\nlogger.add_handler(ElasticsearchHandler(\n    \"http://elasticsearch:9200\",\n    index_prefix=\"myapp\"\n))\n</code></pre>"},{"location":"api/logfile/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Use batching for better performance\nlogger = LogFile(\n    batch_size=100,  # Buffer 100 logs before writing\n    sampling_rate=0.1  # Only log 10% of messages\n)\n\n# Flush buffer manually if needed\nlogger.flush()\n</code></pre>"},{"location":"api/logfile/#security-features","title":"Security Features","text":"<pre><code># Automatically redact sensitive data\nlogger = LogFile(\n    sanitize_keys=[\n        \"password\",\n        \"token\",\n        \"secret\",\n        \"api_key\"\n    ]\n)\n\n# Log will automatically redact sensitive fields\nlogger.log({\n    \"user\": \"john\",\n    \"password\": \"secret123\"  # Will be replaced with \"***REDACTED***\"\n})\n</code></pre>"},{"location":"guide/cloud/","title":"Cloud Integration Guide","text":"<p>LoggingUtil provides robust integration with popular cloud services for log management and analysis.</p>"},{"location":"guide/cloud/#aws-cloudwatch","title":"AWS CloudWatch","text":""},{"location":"guide/cloud/#basic-setup","title":"Basic Setup","text":"<pre><code>from loggingutil import LogFile\nfrom loggingutil.handlers import CloudWatchHandler\n\nlogger = LogFile(\"app.log\")\nlogger.add_handler(CloudWatchHandler(\n    log_group=\"/myapp/prod\",\n    log_stream=\"api-server\",\n    region=\"us-west-2\"\n))\n</code></pre>"},{"location":"guide/cloud/#using-aws-credentials","title":"Using AWS Credentials","text":"<ol> <li> <p>Environment Variables: <pre><code>export AWS_ACCESS_KEY_ID=\"your_access_key\"\nexport AWS_SECRET_ACCESS_KEY=\"your_secret_key\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n</code></pre></p> </li> <li> <p>Direct Configuration: <pre><code>handler = CloudWatchHandler(\n    log_group=\"/myapp/prod\",\n    log_stream=\"api-server\",\n    aws_access_key=\"your_access_key\",\n    aws_secret_key=\"your_secret_key\",\n    region=\"us-west-2\"\n)\n</code></pre></p> </li> <li> <p>IAM Role (recommended for EC2): <pre><code># No credentials needed when using IAM roles\nhandler = CloudWatchHandler(\n    log_group=\"/myapp/prod\",\n    log_stream=\"api-server\"\n)\n</code></pre></p> </li> </ol>"},{"location":"guide/cloud/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use descriptive log group names: <pre><code>log_group = f\"/myapp/{environment}/{component}\"  # e.g., /myapp/prod/api\n</code></pre></p> </li> <li> <p>Dynamic log streams: <pre><code>import socket\nfrom datetime import datetime\n\nlog_stream = f\"{socket.gethostname()}-{datetime.now().strftime('%Y-%m-%d')}\"\n</code></pre></p> </li> <li> <p>Error handling: <pre><code>try:\n    handler = CloudWatchHandler(...)\n    logger.add_handler(handler)\nexcept Exception as e:\n    # Fallback to local logging\n    logger.log_exception(e)\n    logger.add_handler(FileRotatingHandler(\"logs\"))\n</code></pre></p> </li> </ol>"},{"location":"guide/cloud/#elasticsearch","title":"Elasticsearch","text":""},{"location":"guide/cloud/#basic-setup_1","title":"Basic Setup","text":"<pre><code>from loggingutil import LogFile\nfrom loggingutil.handlers import ElasticsearchHandler\n\nlogger = LogFile(\"app.log\")\nlogger.add_handler(ElasticsearchHandler(\n    es_url=\"http://elasticsearch:9200\",\n    index_prefix=\"myapp\"\n))\n</code></pre>"},{"location":"guide/cloud/#authentication","title":"Authentication","text":"<ol> <li> <p>Basic Auth: <pre><code>handler = ElasticsearchHandler(\n    es_url=\"https://elasticsearch:9200\",\n    index_prefix=\"myapp\",\n    auth=(\"user\", \"password\")\n)\n</code></pre></p> </li> <li> <p>API Key: <pre><code>handler = ElasticsearchHandler(\n    es_url=\"https://elasticsearch:9200\",\n    index_prefix=\"myapp\",\n    auth=(\"api_key_id\", \"api_key\")\n)\n</code></pre></p> </li> </ol>"},{"location":"guide/cloud/#index-management","title":"Index Management","text":"<ol> <li> <p>Time-based indices: <pre><code>handler = ElasticsearchHandler(\n    es_url=\"https://elasticsearch:9200\",\n    index_prefix=\"myapp-logs\"  # Results in indices like myapp-logs-2024.03.21\n)\n</code></pre></p> </li> <li> <p>Custom index naming: <pre><code>from datetime import datetime\n\nclass CustomElasticsearchHandler(ElasticsearchHandler):\n    def _get_index_name(self) -&gt; str:\n        env = os.getenv(\"ENVIRONMENT\", \"dev\")\n        date = datetime.now().strftime(\"%Y.%m\")\n        return f\"logs-{env}-{date}\"\n</code></pre></p> </li> </ol>"},{"location":"guide/cloud/#best-practices_1","title":"Best Practices","text":"<ol> <li> <p>Use index templates: <pre><code>{\n  \"index_patterns\": [\"myapp-*\"],\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 1\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"timestamp\": { \"type\": \"date\" },\n      \"level\": { \"type\": \"keyword\" },\n      \"correlation_id\": { \"type\": \"keyword\" },\n      \"message\": { \"type\": \"text\" }\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Configure retention: <pre><code># Using ILM (Index Lifecycle Management)\n{\n  \"policy\": {\n    \"phases\": {\n      \"hot\": {\n        \"actions\": {\n          \"rollover\": {\n            \"max_size\": \"50GB\",\n            \"max_age\": \"30d\"\n          }\n        }\n      },\n      \"delete\": {\n        \"min_age\": \"90d\",\n        \"actions\": {\n          \"delete\": {}\n        }\n      }\n    }\n  }\n}\n</code></pre></p> </li> </ol>"},{"location":"guide/cloud/#multiple-cloud-services","title":"Multiple Cloud Services","text":"<p>You can send logs to multiple cloud services simultaneously:</p> <pre><code>from loggingutil import LogFile\nfrom loggingutil.handlers import (\n    CloudWatchHandler,\n    ElasticsearchHandler,\n    ConsoleHandler\n)\n\nlogger = LogFile(\"app.log\")\n\n# Local console output\nlogger.add_handler(ConsoleHandler(color=True))\n\n# CloudWatch for metrics and alerts\nlogger.add_handler(CloudWatchHandler(\n    log_group=\"/myapp/prod\",\n    log_stream=\"api-server\"\n))\n\n# Elasticsearch for search and analysis\nlogger.add_handler(ElasticsearchHandler(\n    es_url=\"http://elasticsearch:9200\",\n    index_prefix=\"myapp\"\n))\n</code></pre>"},{"location":"guide/cloud/#error-handling-and-fallbacks","title":"Error Handling and Fallbacks","text":"<p>Implement robust error handling for cloud services:</p> <pre><code>class ResilientCloudWatchHandler(CloudWatchHandler):\n    def __init__(self, *args, **kwargs):\n        self.fallback = FileRotatingHandler(\"logs/cloudwatch-fallback\")\n        super().__init__(*args, **kwargs)\n\n    async def handle(self, log_entry: dict):\n        try:\n            await super().handle(log_entry)\n        except Exception as e:\n            print(f\"CloudWatch error: {e}\")\n            await self.fallback.handle(log_entry)\n</code></pre>"},{"location":"guide/cloud/#monitoring-cloud-costs","title":"Monitoring Cloud Costs","text":"<ol> <li> <p>Use sampling to reduce volume: <pre><code>logger = LogFile(\n    sampling_rate=0.1  # Only send 10% of logs\n)\n</code></pre></p> </li> <li> <p>Implement log levels effectively: <pre><code># Development\nlogger.setLevel(LogLevel.DEBUG)\n\n# Production\nlogger.setLevel(LogLevel.INFO)\n</code></pre></p> </li> <li> <p>Use batching for efficiency: <pre><code>handler = CloudWatchHandler(\n    log_group=\"/myapp/prod\",\n    log_stream=\"api-server\",\n    batch_size=100  # Send logs in batches\n)\n</code></pre></p> </li> </ol>"}]}